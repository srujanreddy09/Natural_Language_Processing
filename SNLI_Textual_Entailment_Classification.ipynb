{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras, nltk\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, Concatenate, Dropout, MaxPooling1D, Bidirectional, LSTM, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: \n",
      "['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs: \")\n",
    "print(keras.backend.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SNLI Corpus](https://nlp.stanford.edu/projects/snli/) (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json('data/snli_1.0_train.jsonl', lines=True)\n",
    "validation_df = pd.read_json('data/snli_1.0_dev.jsonl', lines=True)\n",
    "test_df = pd.read_json('data/snli_1.0_test.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotator_labels</th>\n",
       "      <th>captionID</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>pairID</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence1_binary_parse</th>\n",
       "      <th>sentence1_parse</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>sentence2_binary_parse</th>\n",
       "      <th>sentence2_parse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>neutral</td>\n",
       "      <td>3416050480.jpg#4r1n</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>( ( A person ) ( ( is ( ( training ( his horse...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[contradiction]</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>contradiction</td>\n",
       "      <td>3416050480.jpg#4r1c</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>3416050480.jpg#4</td>\n",
       "      <td>entailment</td>\n",
       "      <td>3416050480.jpg#4r1e</td>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...</td>\n",
       "      <td>(ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...</td>\n",
       "      <td>(ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[neutral]</td>\n",
       "      <td>2267923837.jpg#2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2267923837.jpg#2r1n</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>( They ( are ( smiling ( at ( their parents ) ...</td>\n",
       "      <td>(ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[entailment]</td>\n",
       "      <td>2267923837.jpg#2</td>\n",
       "      <td>entailment</td>\n",
       "      <td>2267923837.jpg#2r1e</td>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>( Children ( ( ( smiling and ) waving ) ( at c...</td>\n",
       "      <td>(ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>( There ( ( are children ) present ) )</td>\n",
       "      <td>(ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  annotator_labels         captionID     gold_label               pairID  \\\n",
       "0        [neutral]  3416050480.jpg#4        neutral  3416050480.jpg#4r1n   \n",
       "1  [contradiction]  3416050480.jpg#4  contradiction  3416050480.jpg#4r1c   \n",
       "2     [entailment]  3416050480.jpg#4     entailment  3416050480.jpg#4r1e   \n",
       "3        [neutral]  2267923837.jpg#2        neutral  2267923837.jpg#2r1n   \n",
       "4     [entailment]  2267923837.jpg#2     entailment  2267923837.jpg#2r1e   \n",
       "\n",
       "                                           sentence1  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                              sentence1_binary_parse  \\\n",
       "0  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "1  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "2  ( ( ( A person ) ( on ( a horse ) ) ) ( ( jump...   \n",
       "3  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "4  ( Children ( ( ( smiling and ) waving ) ( at c...   \n",
       "\n",
       "                                     sentence1_parse  \\\n",
       "0  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "1  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "2  (ROOT (S (NP (NP (DT A) (NN person)) (PP (IN o...   \n",
       "3  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
       "4  (ROOT (NP (S (NP (NNP Children)) (VP (VBG smil...   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  A person is training his horse for a competition.   \n",
       "1      A person is at a diner, ordering an omelette.   \n",
       "2                  A person is outdoors, on a horse.   \n",
       "3                  They are smiling at their parents   \n",
       "4                         There are children present   \n",
       "\n",
       "                              sentence2_binary_parse  \\\n",
       "0  ( ( A person ) ( ( is ( ( training ( his horse...   \n",
       "1  ( ( A person ) ( ( ( ( is ( at ( a diner ) ) )...   \n",
       "2  ( ( A person ) ( ( ( ( is outdoors ) , ) ( on ...   \n",
       "3  ( They ( are ( smiling ( at ( their parents ) ...   \n",
       "4             ( There ( ( are children ) present ) )   \n",
       "\n",
       "                                     sentence2_parse  \n",
       "0  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
       "1  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
       "2  (ROOT (S (NP (DT A) (NN person)) (VP (VBZ is) ...  \n",
       "3  (ROOT (S (NP (PRP They)) (VP (VBP are) (VP (VB...  \n",
       "4  (ROOT (S (NP (EX There)) (VP (VBP are) (NP (NN...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Embedding Dimensions\n",
    "embedding_dim = 300\n",
    "# Number of Epochs\n",
    "nb_epochs = 100\n",
    "# Maximum length of a sentence\n",
    "max_seq_len = 100\n",
    "# Number of classes\n",
    "nb_classes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is training his horse for a competition.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is at a diner, ordering an omelette.</td>\n",
       "      <td>contradiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A person on a horse jumps over a broken down a...</td>\n",
       "      <td>A person is outdoors, on a horse.</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>They are smiling at their parents</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Children smiling and waving at camera</td>\n",
       "      <td>There are children present</td>\n",
       "      <td>entailment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  A person on a horse jumps over a broken down a...   \n",
       "1  A person on a horse jumps over a broken down a...   \n",
       "2  A person on a horse jumps over a broken down a...   \n",
       "3              Children smiling and waving at camera   \n",
       "4              Children smiling and waving at camera   \n",
       "\n",
       "                                          hypothesis          label  \n",
       "0  A person is training his horse for a competition.        neutral  \n",
       "1      A person is at a diner, ordering an omelette.  contradiction  \n",
       "2                  A person is outdoors, on a horse.     entailment  \n",
       "3                  They are smiling at their parents        neutral  \n",
       "4                         There are children present     entailment  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping unwanted columns and renaming the remaining columns\n",
    "def drop_columns(df):\n",
    "    required_columns = [\"sentence1\", \"sentence2\", \"gold_label\"]\n",
    "    return_df = df[required_columns]\n",
    "    return_df.columns = [\"premise\", \"hypothesis\", \"label\"]\n",
    "    # Dropping data with multiple labels\n",
    "    return_df = return_df[return_df[\"label\"] != \"-\"]\n",
    "    return return_df\n",
    "\n",
    "train_df = drop_columns(train_df)\n",
    "validation_df = drop_columns(validation_df)\n",
    "test_df = drop_columns(test_df)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAD4CAYAAADFAawfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(LabelEncoder().fit_transform(train_df[\"label\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Examples are equally distributed among the three classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an embedding matrix from a word2vec model trained on premise texts and hypothesis texts\n",
    "def create_embedding_matrix(corpus, tokenizer, vocabulary_size, embedding_dimension = embedding_dim):\n",
    "    # Train a word2vec model\n",
    "    wvmodel = Word2Vec(corpus, size = embedding_dimension)\n",
    "    \n",
    "    # Create a vocabulary dictionary\n",
    "    vocabulary = {}\n",
    "    for key, value in wvmodel.wv.vocab.items():\n",
    "        vocabulary[key] = value\n",
    "    \n",
    "    # Create an embedding matrix with embedding vectors from the word2vec model\n",
    "    embedding_matrix = np.zeros((vocabulary_size, embedding_dimension))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if vocabulary.get(word) is not None:\n",
    "            embedding_matrix[i] = wvmodel[word]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create corpi\n",
    "premise_corpus = [nltk.word_tokenize(sent) for sent in train_df[\"premise\"]]\n",
    "hypothesis_corpus = [nltk.word_tokenize(sent) for sent in train_df[\"hypothesis\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize corpi\n",
    "premise_tokenizer = Tokenizer(oov_token = \"oov_tok\")\n",
    "premise_tokenizer.fit_on_texts(premise_corpus)\n",
    "premise_vocab_size = len(premise_tokenizer.word_index) + 1\n",
    "\n",
    "hypothesis_tokenizer = Tokenizer(oov_token = \"oov_tok\")\n",
    "hypothesis_tokenizer.fit_on_texts(hypothesis_corpus)\n",
    "hypothesis_vocab_size = len(hypothesis_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert text to sequences\n",
    "premise_seqs = premise_tokenizer.texts_to_sequences(premise_corpus)\n",
    "hypothesis_seqs = hypothesis_tokenizer.texts_to_sequences(hypothesis_corpus)\n",
    "\n",
    "# Pad the tokenised sequences\n",
    "padded_premise_seqs = pad_sequences(premise_seqs, maxlen=max_seq_len, padding = \"pre\")\n",
    "padded_hypothesis_seqs = pad_sequences(hypothesis_seqs, maxlen=max_seq_len, padding = \"pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare validation dataset\n",
    "premise_seqs_val = premise_tokenizer.texts_to_sequences([sent for sent in validation_df[\"premise\"]])\n",
    "padded_premise_seqs_val = pad_sequences(premise_seqs_val, maxlen=max_seq_len, padding = \"pre\")\n",
    "\n",
    "hypothesis_seqs_val = hypothesis_tokenizer.texts_to_sequences([sent for sent in validation_df[\"hypothesis\"]])\n",
    "padded_hypothesis_seqs_val = pad_sequences(hypothesis_seqs_val, maxlen=max_seq_len, padding = \"pre\")\n",
    "\n",
    "# Prepare test dataset\n",
    "premise_seqs_test = premise_tokenizer.texts_to_sequences([sent for sent in test_df[\"premise\"]])\n",
    "padded_premise_seqs_test = pad_sequences(premise_seqs_test, maxlen=max_seq_len, padding = \"pre\")\n",
    "\n",
    "hypothesis_seqs_test = hypothesis_tokenizer.texts_to_sequences([sent for sent in test_df[\"hypothesis\"]])\n",
    "padded_hypothesis_seqs_test = pad_sequences(hypothesis_seqs_test, maxlen=max_seq_len, padding = \"pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot encode Labels\n",
    "one_enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "one_enc.fit(np.array(train_df[\"label\"]).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(549367, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = one_enc.transform(np.array(train_df[\"label\"]).reshape(-1, 1))\n",
    "val_labels = one_enc.transform(np.array(validation_df[\"label\"]).reshape(-1, 1))\n",
    "test_labels = one_enc.transform(np.array(test_df[\"label\"]).reshape(-1, 1))\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input layers\n",
    "premise_inputs = Input(shape = (max_seq_len, ))\n",
    "hypothesis_inputs = Input(shape = (max_seq_len, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating embedding matrices for premise and hypothesis corpi\n",
    "premise_embedding_matrix = create_embedding_matrix(premise_corpus, \n",
    "                                                   premise_tokenizer, \n",
    "                                                   premise_vocab_size)\n",
    "\n",
    "hypothesis_embedding_matrix = create_embedding_matrix(hypothesis_corpus, \n",
    "                                                   hypothesis_tokenizer, \n",
    "                                                   hypothesis_vocab_size)\n",
    "\n",
    "# Creating embedding layers with embedding matrices as weights\n",
    "premise_embedding_layer = Embedding(input_dim=premise_vocab_size, \n",
    "                                    output_dim=embedding_dim, \n",
    "                                    input_length=max_seq_len, \n",
    "                                    weights = [premise_embedding_matrix], \n",
    "                                    trainable = False)(premise_inputs)\n",
    "hypothesis_embedding_layer = Embedding(input_dim=hypothesis_vocab_size, \n",
    "                                    output_dim=embedding_dim, \n",
    "                                    input_length=max_seq_len, \n",
    "                                    weights = [hypothesis_embedding_matrix], \n",
    "                                    trainable = False)(hypothesis_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture for premise texts\n",
    "premise_model = Conv1D(64, kernel_size=3, activation = \"relu\")(premise_embedding_layer)\n",
    "premise_model = BatchNormalization()(premise_model)\n",
    "premise_model = Conv1D(64, kernel_size=3, activation = \"relu\")(premise_model)\n",
    "premise_model = BatchNormalization()(premise_model)\n",
    "premise_model = MaxPooling1D(2)(premise_model)\n",
    "premise_model = Dropout(0.3)(premise_model)\n",
    "premise_model = Bidirectional(LSTM(50))(premise_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model architecture for hypothesis texts\n",
    "hypothesis_model = Conv1D(64, kernel_size=3, activation = \"relu\")(hypothesis_embedding_layer)\n",
    "hypothesis_model = BatchNormalization()(hypothesis_model)\n",
    "hypothesis_model = Conv1D(64, kernel_size=3, activation = \"relu\")(hypothesis_model)\n",
    "hypothesis_model = BatchNormalization()(hypothesis_model)\n",
    "hypothesis_model = MaxPooling1D(2)(hypothesis_model)\n",
    "hypothesis_model = Dropout(0.3)(hypothesis_model)\n",
    "hypothesis_model = Bidirectional(LSTM(50))(hypothesis_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatening the premise and hypothesis models\n",
    "x = Concatenate()([premise_model, hypothesis_model])\n",
    "# DNN\n",
    "x = Dense(256, activation = \"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(nb_classes, activation = \"softmax\")(x)\n",
    "model = Model([premise_inputs, hypothesis_inputs], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 100, 300)     6903000     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 300)     11030400    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 98, 64)       57664       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 98, 64)       57664       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 98, 64)       256         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 98, 64)       256         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 96, 64)       12352       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 96, 64)       12352       batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 96, 64)       256         conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 96, 64)       256         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 48, 64)       0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 48, 64)       0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 48, 64)       0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 48, 64)       0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 100)          46000       dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_4 (Bidirectional) (None, 100)          46000       dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 200)          0           bidirectional_3[0][0]            \n",
      "                                                                 bidirectional_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 256)          51456       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 256)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 3)            771         dropout_15[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 18,218,683\n",
      "Trainable params: 284,771\n",
      "Non-trainable params: 17,933,912\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Early stop callback\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", patience = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 549367 samples, validate on 9842 samples\n",
      "Epoch 1/100\n",
      "549367/549367 [==============================] - 1516s 3ms/step - loss: 0.8881 - acc: 0.5885 - val_loss: 0.8393 - val_acc: 0.6221\n",
      "Epoch 2/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.8187 - acc: 0.6316 - val_loss: 0.8059 - val_acc: 0.6369\n",
      "Epoch 3/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.7905 - acc: 0.6485 - val_loss: 0.7980 - val_acc: 0.6443\n",
      "Epoch 4/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.7720 - acc: 0.6579 - val_loss: 0.7704 - val_acc: 0.6656\n",
      "Epoch 5/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.7566 - acc: 0.6666 - val_loss: 0.7515 - val_acc: 0.6760\n",
      "Epoch 6/100\n",
      "549367/549367 [==============================] - 1507s 3ms/step - loss: 0.7432 - acc: 0.6742 - val_loss: 0.7454 - val_acc: 0.6775\n",
      "Epoch 7/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.7328 - acc: 0.6799 - val_loss: 0.7297 - val_acc: 0.6859\n",
      "Epoch 8/100\n",
      "549367/549367 [==============================] - 1507s 3ms/step - loss: 0.7239 - acc: 0.6849 - val_loss: 0.7310 - val_acc: 0.6877\n",
      "Epoch 9/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.7153 - acc: 0.6902 - val_loss: 0.7222 - val_acc: 0.6911\n",
      "Epoch 10/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.7079 - acc: 0.6934 - val_loss: 0.7235 - val_acc: 0.6931\n",
      "Epoch 11/100\n",
      "549367/549367 [==============================] - 1507s 3ms/step - loss: 0.7016 - acc: 0.6966 - val_loss: 0.7191 - val_acc: 0.6942\n",
      "Epoch 12/100\n",
      "549367/549367 [==============================] - 1507s 3ms/step - loss: 0.6965 - acc: 0.6992 - val_loss: 0.7147 - val_acc: 0.6949\n",
      "Epoch 13/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6912 - acc: 0.7022 - val_loss: 0.7119 - val_acc: 0.6950\n",
      "Epoch 14/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6865 - acc: 0.7042 - val_loss: 0.7047 - val_acc: 0.6967\n",
      "Epoch 15/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6825 - acc: 0.7066 - val_loss: 0.7113 - val_acc: 0.7011\n",
      "Epoch 16/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6784 - acc: 0.7087 - val_loss: 0.7056 - val_acc: 0.7001\n",
      "Epoch 17/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6749 - acc: 0.7109 - val_loss: 0.7001 - val_acc: 0.6986\n",
      "Epoch 18/100\n",
      "549367/549367 [==============================] - 1507s 3ms/step - loss: 0.6711 - acc: 0.7123 - val_loss: 0.7026 - val_acc: 0.7050\n",
      "Epoch 19/100\n",
      "549367/549367 [==============================] - 1507s 3ms/step - loss: 0.6684 - acc: 0.7137 - val_loss: 0.7000 - val_acc: 0.7012\n",
      "Epoch 20/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6661 - acc: 0.7154 - val_loss: 0.6958 - val_acc: 0.7039\n",
      "Epoch 21/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6627 - acc: 0.7170 - val_loss: 0.6962 - val_acc: 0.7013\n",
      "Epoch 22/100\n",
      "549367/549367 [==============================] - 1507s 3ms/step - loss: 0.6601 - acc: 0.7173 - val_loss: 0.6904 - val_acc: 0.7066\n",
      "Epoch 23/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6569 - acc: 0.7200 - val_loss: 0.6957 - val_acc: 0.7053\n",
      "Epoch 24/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6543 - acc: 0.7207 - val_loss: 0.6968 - val_acc: 0.7023\n",
      "Epoch 25/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6518 - acc: 0.7226 - val_loss: 0.6909 - val_acc: 0.7086\n",
      "Epoch 26/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6481 - acc: 0.7251 - val_loss: 0.6806 - val_acc: 0.7138\n",
      "Epoch 27/100\n",
      "549367/549367 [==============================] - 1508s 3ms/step - loss: 0.6424 - acc: 0.7280 - val_loss: 0.6744 - val_acc: 0.7157\n",
      "Epoch 28/100\n",
      "549367/549367 [==============================] - 1503s 3ms/step - loss: 0.6201 - acc: 0.7382 - val_loss: 0.6647 - val_acc: 0.7246\n",
      "Epoch 34/100\n",
      "549367/549367 [==============================] - 1505s 3ms/step - loss: 0.5924 - acc: 0.7520 - val_loss: 0.6483 - val_acc: 0.7311\n"
     ]
    }
   ],
   "source": [
    "# Fitting the data\n",
    "history = model.fit(x = [padded_premise_seqs, padded_hypothesis_seqs], \n",
    "                    y = train_labels, \n",
    "                    validation_data = ([padded_premise_seqs_val, padded_hypothesis_seqs_val], val_labels),\n",
    "                    shuffle = True,\n",
    "                    epochs = nb_epochs,\n",
    "                    batch_size = 64,\n",
    "                    callbacks = [early_stop]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9824/9824 [==============================] - 12s 1ms/step\n",
      "Accuracy on test set :  73.402\n"
     ]
    }
   ],
   "source": [
    "_, acc = model.evaluate([padded_premise_seqs_test, padded_hypothesis_seqs_test], test_labels)\n",
    "print(\"Accuracy on test set : \", round(acc*100, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict([padded_premise_seqs_test, padded_hypothesis_seqs_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.80      0.69      0.74      3237\n",
      "   entailment       0.73      0.78      0.75      3368\n",
      "      neutral       0.69      0.73      0.71      3219\n",
      "\n",
      "     accuracy                           0.73      9824\n",
      "    macro avg       0.74      0.73      0.73      9824\n",
      " weighted avg       0.74      0.73      0.73      9824\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.argmax(test_labels, axis = 1), np.argmax(test_pred, axis = 1), target_names=one_enc.categories_[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

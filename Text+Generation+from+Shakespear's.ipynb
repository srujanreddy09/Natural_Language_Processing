{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dense, Flatten, LSTM, Bidirectional, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: \n",
      "['/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1']\n"
     ]
    }
   ],
   "source": [
    "print(\"GPUs: \")\n",
    "print(keras.backend.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epochs = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40,000 lines of Shakespeare from a variety of Shakespeare's plays. \n",
    "Featured in Andrej Karpathy's blog post 'The Unreasonable Effectiveness of Recurrent Neural Networks': http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"input.txt\") as f:\n",
    "    corpus = list(f)\n",
    "# Removing line break characters\n",
    "corpus = [sent[:-1] for sent in corpus]\n",
    "# Removing empty strings\n",
    "corpus = [sent for sent in corpus if sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizing the data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting text to input sequences\n",
    "input_sequences = []\n",
    "for sent in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([sent])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[88, 269],\n",
       " [139, 35],\n",
       " [139, 35, 969],\n",
       " [139, 35, 969, 143],\n",
       " [139, 35, 969, 143, 668]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example input sequences\n",
    "input_sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Length of the longest sequence\n",
    "max_seq_len = max([len(seq) for seq in input_sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Padding the sequences\n",
    "padded_input_sequences = pad_sequences(input_sequences, maxlen=max_seq_len, padding = \"pre\")\n",
    "# Splitting the sequences into features and labels\n",
    "train_data = padded_input_sequences[:, :-1]\n",
    "train_labels = padded_input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-Hot encoding the labels\n",
    "train_labels = to_categorical(train_labels, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=max_seq_len-1))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(vocab_size, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 15, 100)           1263300   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 200)               800       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 12633)             2539233   \n",
      "=================================================================\n",
      "Total params: 3,964,133\n",
      "Trainable params: 3,963,733\n",
      "Non-trainable params: 400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Early stop callback\n",
    "early_stop = EarlyStopping(monitor = \"loss\", patience = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fitting the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "171312/171312 [==============================] - 212s 1ms/step - loss: 6.4321 - acc: 0.0859\n",
      "Epoch 2/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 5.6903 - acc: 0.1177\n",
      "Epoch 3/100\n",
      "171312/171312 [==============================] - 211s 1ms/step - loss: 5.2180 - acc: 0.1429\n",
      "Epoch 4/100\n",
      "171312/171312 [==============================] - 211s 1ms/step - loss: 4.7588 - acc: 0.1752\n",
      "Epoch 5/100\n",
      "171312/171312 [==============================] - 211s 1ms/step - loss: 4.3394 - acc: 0.2118\n",
      "Epoch 6/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 3.9863 - acc: 0.2519\n",
      "Epoch 7/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 3.7109 - acc: 0.2857\n",
      "Epoch 8/100\n",
      "171312/171312 [==============================] - 215s 1ms/step - loss: 3.4990 - acc: 0.3150\n",
      "Epoch 9/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 3.3267 - acc: 0.3411\n",
      "Epoch 10/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 3.1816 - acc: 0.3631\n",
      "Epoch 11/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 3.0553 - acc: 0.3851\n",
      "Epoch 12/100\n",
      "171312/171312 [==============================] - 211s 1ms/step - loss: 2.9447 - acc: 0.4024\n",
      "Epoch 13/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 2.8489 - acc: 0.4192\n",
      "Epoch 14/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 2.7560 - acc: 0.4355\n",
      "Epoch 15/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 2.6796 - acc: 0.4491\n",
      "Epoch 16/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 2.6089 - acc: 0.4607\n",
      "Epoch 17/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 2.5404 - acc: 0.4751\n",
      "Epoch 18/100\n",
      "171312/171312 [==============================] - 207s 1ms/step - loss: 2.4820 - acc: 0.4851\n",
      "Epoch 19/100\n",
      "171312/171312 [==============================] - 208s 1ms/step - loss: 2.4242 - acc: 0.4970\n",
      "Epoch 20/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 2.3740 - acc: 0.5052\n",
      "Epoch 21/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 2.3250 - acc: 0.5159\n",
      "Epoch 22/100\n",
      "171312/171312 [==============================] - 214s 1ms/step - loss: 2.2833 - acc: 0.5235\n",
      "Epoch 23/100\n",
      "171312/171312 [==============================] - 214s 1ms/step - loss: 2.2384 - acc: 0.5325\n",
      "Epoch 24/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 2.2009 - acc: 0.5404\n",
      "Epoch 25/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 2.1666 - acc: 0.5459\n",
      "Epoch 26/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 2.1285 - acc: 0.5535\n",
      "Epoch 27/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 2.0990 - acc: 0.5614\n",
      "Epoch 28/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 2.0669 - acc: 0.5667\n",
      "Epoch 29/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 2.0416 - acc: 0.5725\n",
      "Epoch 30/100\n",
      "171312/171312 [==============================] - 211s 1ms/step - loss: 2.0145 - acc: 0.5768\n",
      "Epoch 31/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 1.9893 - acc: 0.5832\n",
      "Epoch 32/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 1.9656 - acc: 0.5872\n",
      "Epoch 33/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 1.9427 - acc: 0.5920\n",
      "Epoch 34/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 1.9251 - acc: 0.5954\n",
      "Epoch 35/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 1.9025 - acc: 0.5995\n",
      "Epoch 36/100\n",
      "171312/171312 [==============================] - 212s 1ms/step - loss: 1.8864 - acc: 0.6030\n",
      "Epoch 37/100\n",
      "171312/171312 [==============================] - 215s 1ms/step - loss: 1.8631 - acc: 0.6085\n",
      "Epoch 38/100\n",
      "171312/171312 [==============================] - 210s 1ms/step - loss: 1.8464 - acc: 0.6113\n",
      "Epoch 39/100\n",
      "171312/171312 [==============================] - 206s 1ms/step - loss: 1.8283 - acc: 0.6162\n",
      "Epoch 40/100\n",
      "171312/171312 [==============================] - 204s 1ms/step - loss: 1.8160 - acc: 0.6180\n",
      "Epoch 41/100\n",
      "171312/171312 [==============================] - 206s 1ms/step - loss: 1.7980 - acc: 0.6223\n",
      "Epoch 42/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 1.7827 - acc: 0.6247\n",
      "Epoch 43/100\n",
      "171312/171312 [==============================] - 208s 1ms/step - loss: 1.7707 - acc: 0.6269\n",
      "Epoch 44/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 1.7590 - acc: 0.6297\n",
      "Epoch 45/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 1.7432 - acc: 0.6326\n",
      "Epoch 46/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 1.7334 - acc: 0.6349\n",
      "Epoch 47/100\n",
      "171312/171312 [==============================] - 209s 1ms/step - loss: 1.7214 - acc: 0.6376\n",
      "Epoch 48/100\n",
      "171312/171312 [==============================] - 225s 1ms/step - loss: 1.5579 - acc: 0.6703\n",
      "Epoch 68/100\n",
      "171312/171312 [==============================] - 225s 1ms/step - loss: 1.5506 - acc: 0.6740\n",
      "Epoch 69/100\n",
      "171312/171312 [==============================] - 229s 1ms/step - loss: 1.5487 - acc: 0.6744\n",
      "Epoch 70/100\n",
      "171312/171312 [==============================] - 232s 1ms/step - loss: 1.5405 - acc: 0.6759\n",
      "Epoch 71/100\n",
      "171312/171312 [==============================] - 231s 1ms/step - loss: 1.5398 - acc: 0.6754\n",
      "Epoch 72/100\n",
      "171312/171312 [==============================] - 230s 1ms/step - loss: 1.5310 - acc: 0.6785\n",
      "Epoch 73/100\n",
      "171312/171312 [==============================] - 227s 1ms/step - loss: 1.5288 - acc: 0.6779\n",
      "Epoch 74/100\n",
      "171312/171312 [==============================] - 221s 1ms/step - loss: 1.5231 - acc: 0.6793\n",
      "Epoch 75/100\n",
      "171312/171312 [==============================] - 223s 1ms/step - loss: 1.5162 - acc: 0.6809\n",
      "Epoch 76/100\n",
      "171312/171312 [==============================] - 239s 1ms/step - loss: 1.5144 - acc: 0.6810\n",
      "Epoch 77/100\n",
      "171312/171312 [==============================] - 235s 1ms/step - loss: 1.5100 - acc: 0.6822\n",
      "Epoch 78/100\n",
      "171312/171312 [==============================] - 226s 1ms/step - loss: 1.5064 - acc: 0.6832\n",
      "Epoch 79/100\n",
      "171312/171312 [==============================] - 208s 1ms/step - loss: 1.5037 - acc: 0.6836\n",
      "Epoch 80/100\n",
      "171312/171312 [==============================] - 208s 1ms/step - loss: 1.4986 - acc: 0.6851\n",
      "Epoch 81/100\n",
      "171312/171312 [==============================] - 202s 1ms/step - loss: 1.4936 - acc: 0.6849\n",
      "Epoch 82/100\n",
      "171312/171312 [==============================] - 197s 1ms/step - loss: 1.4911 - acc: 0.6861\n",
      "Epoch 83/100\n",
      "171312/171312 [==============================] - 197s 1ms/step - loss: 1.4882 - acc: 0.6861\n",
      "Epoch 84/100\n",
      "171312/171312 [==============================] - 197s 1ms/step - loss: 1.4836 - acc: 0.6883\n",
      "Epoch 85/100\n",
      "171312/171312 [==============================] - 197s 1ms/step - loss: 1.4797 - acc: 0.6894\n",
      "Epoch 86/100\n",
      "171312/171312 [==============================] - 197s 1ms/step - loss: 1.4801 - acc: 0.6882\n",
      "Epoch 87/100\n",
      "171312/171312 [==============================] - 198s 1ms/step - loss: 1.4724 - acc: 0.6909\n",
      "Epoch 88/100\n",
      "171312/171312 [==============================] - 199s 1ms/step - loss: 1.4733 - acc: 0.6902\n",
      "Epoch 89/100\n",
      "171312/171312 [==============================] - 201s 1ms/step - loss: 1.4660 - acc: 0.6921\n",
      "Epoch 90/100\n",
      "171312/171312 [==============================] - 203s 1ms/step - loss: 1.4639 - acc: 0.6919\n",
      "Epoch 91/100\n",
      "171312/171312 [==============================] - 197s 1ms/step - loss: 1.4639 - acc: 0.6921\n",
      "Epoch 92/100\n",
      "171312/171312 [==============================] - 202s 1ms/step - loss: 1.4579 - acc: 0.6931\n",
      "Epoch 93/100\n",
      "171312/171312 [==============================] - 207s 1ms/step - loss: 1.4579 - acc: 0.6942\n",
      "Epoch 94/100\n",
      "171312/171312 [==============================] - 204s 1ms/step - loss: 1.4557 - acc: 0.6936\n",
      "Epoch 95/100\n",
      "171312/171312 [==============================] - 203s 1ms/step - loss: 1.4550 - acc: 0.6944\n",
      "Epoch 96/100\n",
      "171312/171312 [==============================] - 207s 1ms/step - loss: 1.4483 - acc: 0.6951\n",
      "Epoch 97/100\n",
      "171312/171312 [==============================] - 198s 1ms/step - loss: 1.4428 - acc: 0.6960\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171312/171312 [==============================] - 206s 1ms/step - loss: 1.4475 - acc: 0.6963\n",
      "Epoch 99/100\n",
      "171312/171312 [==============================] - 207s 1ms/step - loss: 1.4430 - acc: 0.6957\n",
      "Epoch 100/100\n",
      "171312/171312 [==============================] - ETA: 0s - loss: 1.4412 - acc: 0.696 - 208s 1ms/step - loss: 1.4412 - acc: 0.6962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2adc09577e80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, epochs=nb_epochs, batch_size=batch_size, callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generating a sample shakespeare play**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_text = \"Brevity is the soul of wit\"\n",
    "# Setting the length of the play\n",
    "num_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the next word from the seed text\n",
    "for _ in range(num_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding = \"pre\")\n",
    "    predicted = model.predict_classes(token_list, verbose = 0)\n",
    "    output = \"\"\n",
    "    for word, idx in tokenizer.word_index.items():\n",
    "        if idx == predicted:\n",
    "            output = word\n",
    "            break\n",
    "    seed_text += \" \" + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brevity is the soul of wit bid it come not better it but first burthenous some sprays crooked horseman's gown talked month title title month message\n"
     ]
    }
   ],
   "source": [
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
